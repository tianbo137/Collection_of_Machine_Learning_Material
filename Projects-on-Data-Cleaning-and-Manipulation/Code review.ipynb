{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "\n",
    "The dataset we will be working along with is Seattle Public Library Inventory dataset. We have downloaded the dataset from the source and perfomed cleaning operation before uploading into the data. Post that we use pymysql to connect to the database and perform operation like recommending books to user based on the genre and find the book in the inventory based on the authors name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the essential libraries\n",
    "\n",
    "import pandas as pd (#pandas dataframe)\n",
    "import pymysql (#python package to connect to aws instance)\n",
    "import re (#regular expression to filter our columns)\n",
    "import matplotlib.pyplot as plt,%matplotlib inline (#python visualization libraries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using pandas to read the csv file of library inventory\n",
    "df=pd.read_csv('Library_Collection_Inventory.csv')\n",
    "\n",
    "c1=pd.read_csv('../input/seattle-library-checkout-records/Checkouts_By_Title_Data_Lens_2005.csv')\n",
    "c2=pd.read_csv('../input/seattle-library-checkout-records/Checkouts_By_Title_Data_Lens_2006.csv')\n",
    "c3=pd.read_csv('../input/seattle-library-checkout-records/Checkouts_By_Title_Data_Lens_2007.csv')\n",
    "\n",
    "#cleaning the checkout column to parse it as dates\n",
    "\n",
    "c1.CheckoutDateTime.replace(to_replace=r'PM', value=' ', regex=True,inplace=True)\n",
    "ab=c1['CheckoutDateTime'].str.split(\" \",n=1,expand=True)\n",
    "c1['Date']=ab[0]\n",
    "c1['Date']=pd.to_datetime(c1['Date'])\n",
    "\n",
    "c2.CheckoutDateTime.replace(to_replace=r'PM', value=' ', regex=True,inplace=True)\n",
    "ab=c2['CheckoutDateTime'].str.split(\" \",n=1,expand=True)\n",
    "c2['Date']=ab[0]\n",
    "c2['Date']=pd.to_datetime(c2['Date'])\n",
    "c2.drop('CheckoutDateTime',axis=1,inplace=True)\n",
    "\n",
    "c3.CheckoutDateTime.replace(to_replace=r'PM', value=' ', regex=True,inplace=True)\n",
    "ab=c3['CheckoutDateTime'].str.split(\" \",n=1,expand=True)\n",
    "c3['Date']=ab[0]\n",
    "c3['Date']=pd.to_datetime(c3['Date'])\n",
    "c3.drop('CheckoutDateTime',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging all the files\n",
    "f=pd.concat([c1,c2])\n",
    "f=pd.concat([f,c3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing join operations on f and library inventory to get if book was in print or media format\n",
    "\n",
    "new=pd.DataFrame(df[['Title','Author','Subjects','ItemType']])\n",
    "temp=pd.DataFrame(f['BibNumber'])\n",
    "dd=pd.read_csv('../input/seattle-library-checkout-records/Integrated_Library_System__ILS__Data_Dictionary.csv')\n",
    "\n",
    "new=pd.merge(left=f,right=pd.DataFrame(dd[['Code','Format Group']]),left_on='ItemType',right_on='Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing join operations on new and library inventory to get author, tile, subjects and Itemtype details\n",
    "\n",
    "a = df.drop_duplicates(subset='BibNum', keep='last').reset_index()\n",
    "\n",
    "new=pd.merge(left=new,right=pd.DataFrame(a[['BibNum','Title','Author','Subjects']]),left_on='BibNumber',right_on='BibNum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Unwanted columns\n",
    "\n",
    "new.drop(['BibNum','Collection','CallNumber','ItemBarcode'],axis=1,inplace=True)\n",
    "new.drop(['Code'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using date column to find new paramters like year month and dayofweek \n",
    "\n",
    "new['Year']=new['Date'].dt.year\n",
    "new['Month']=new['Date'].dt.month\n",
    "new['Week']=new['Date'].dt.weekofyear\n",
    "new['DayofWeek']=new['Date'].dt.dayofweek\n",
    "\n",
    "#Removing date column as we have segreated its information into new columns\n",
    "\n",
    "new.drop('Date',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have to divide our dataset into different categories, but the subject column does not give us proper value\n",
    "#We will separate this by identifying top occuring genres using regular expression and use them\n",
    "#below is the code that plot the top occuring genres in subjects columns\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,8)) #plotting\n",
    "\n",
    "#plot chart\n",
    "ax1 = plt.subplot(121, aspect='equal')\n",
    "\n",
    "#save top 10 occuring genres in 'a'\n",
    "a=pd.DataFrame(pd.Series(' '.join(map(str,df['Subjects'])).lower().split()).value_counts()[:10],columns=['total'])\n",
    "\n",
    "#plot 'a' as pie chart to show the % distribution\n",
    "\n",
    "a.plot(kind='pie', y = 'total', ax=ax1, autopct='%1.1f%%', \n",
    "startangle=90, shadow=False, labels=a.index, legend = False, fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use the 4 most occuring genres and\n",
    "#Add them in the Subject columns\n",
    "\n",
    "#Adding the columns and intializing them as 0\n",
    "cols=['fiction','mystery','drama','literature']\n",
    "for col in cols: a[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use below script to find if the new genres columns created exist in each row and if they do\n",
    "#make its value 1\n",
    "\n",
    "#Since we have 5lakh rows we will perform operation in parts\n",
    "#Using iterrows to get each row in pandas dataframe\n",
    "\n",
    "\n",
    "for index,row in pd.DataFrame(a[(a.index>50000) & (a.index<=100000)].Subjects).iterrows():\n",
    "   \n",
    "    for col in cols:\n",
    "        if re.search(col,str(row['Subjects']).lower()):\n",
    "            \n",
    "            a.ix[index, col] = 1\n",
    "\n",
    "\n",
    "for index,row in pd.DataFrame(a[(a.index>100000) & (a.index<=200000)].Subjects).iterrows():\n",
    "   \n",
    "    for col in cols:\n",
    "        if re.search(col,str(row['Subjects']).lower()):\n",
    "            \n",
    "            a.ix[index, col] = 1\n",
    "\n",
    "\n",
    "for index,row in pd.DataFrame(a[(a.index>200000) & (a.index<=300000)].Subjects).iterrows():\n",
    "   \n",
    "    for col in cols:\n",
    "        if re.search(col,str(row['Subjects']).lower()):\n",
    "            \n",
    "            a.ix[index, col] = 1\n",
    "            \n",
    "\n",
    "for index,row in pd.DataFrame(a[(a.index>300000) & (a.index<=400000)].Subjects).iterrows():\n",
    "   \n",
    "    for col in cols:\n",
    "        if re.search(col,str(row['Subjects']).lower()):\n",
    "            \n",
    "            a.ix[index, col] = 1\n",
    "            \n",
    "\n",
    "for index,row in pd.DataFrame(a[(a.index>400000) & (a.index<=500000)].Subjects).iterrows():\n",
    "   \n",
    "    for col in cols:\n",
    "        if re.search(col,str(row['Subjects']).lower()):\n",
    "            \n",
    "            a.ix[index, col] = 1\n",
    "            \n",
    "\n",
    "for index,row in pd.DataFrame(a[(a.index>500000) & (a.index<=584391)].Subjects).iterrows():\n",
    "   \n",
    "    for col in cols:\n",
    "        if re.search(col,str(row['Subjects']).lower()):\n",
    "            \n",
    "            a.ix[index, col] = 1            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will map the new columns in our checkout dataset\n",
    "\n",
    "new=pd.merge(left=new,right=pd.DataFrame(a[['BibNum','fiction','mystery','drama','literature']]),\n",
    "             left_on='BibNumber',right_on='BibNum')\n",
    "\n",
    "new.to_csv('Checkout.csv')\n",
    "a.to_csv('Inventory.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will no upload our clean data into the databse\n",
    "\n",
    "#BEFORE that below is the conf file used to setup nodes and establish connections between them\n",
    "\n",
    "[mysqld]\n",
    "datadir=/var/lib/mysql\n",
    "socket=/var/lib/mysql/mysql.sock\n",
    "bind-address=0.0.0.0\n",
    "user=mysql\n",
    "\n",
    "default_storage_engine=InnoDB\n",
    "innodb_autoinc_lock_mode=2\n",
    "innodb_flush_log_at_trx_commit=0\n",
    "innodb_buffer_pool_size=128M\n",
    "\n",
    "binlog_format=ROW\n",
    "log-error=/var/log/mysqld.log\n",
    "\n",
    "[galera]\n",
    "wsrep_on=ON\n",
    "wsrep_provider=/usr/lib/galera/libgalera_smm.so\n",
    "\n",
    "wsrep_node_name='galera3'\n",
    "\n",
    "wsrep_node_address=\"172.31.41.172\"\n",
    "wsrep_cluster_name='galera-training'\n",
    "wsrep_cluster_address=gcomm://172.31.32.84,172.31.42.23,172.31.41.172\n",
    "\n",
    "wsrep_provider_options=\"gcache.size=300M; gcache.page_size=300M\"\n",
    "wsrep_slave_threads=4\n",
    "wsrep_sst_method=rsync\n",
    "\n",
    "\n",
    "\n",
    "#We have already trasnfered our cleaned csv file to aws instance using WINSCP\n",
    "\n",
    "#Next we will create a database and a table in SQL to import this data\n",
    "\n",
    "create database project;\n",
    "\n",
    "create table checkout(\n",
    "BibNumber int(20), \n",
    "ItemType varchar(10),\n",
    "Format Group varchar(10),\n",
    "Title varchar(150),\n",
    "Author varchar(150), \n",
    "Subjects varchar(150),\n",
    "Year int(10), \n",
    "Month int(10), \n",
    "Week int(10), \n",
    "DayofWeek int(5),  \n",
    "fiction int(5), \n",
    "mystery int(5),\n",
    "drama, int(5) \n",
    "literature int(5)\n",
    "\n",
    ");\n",
    "\n",
    "create table inventory(\n",
    "BibNum int(20), \n",
    "Title varchar(150),\n",
    "Author varchar(150),\n",
    "Publisher varchar(150),\n",
    "Subjects varchar(150),\n",
    "ItemType varchar(10),\n",
    "\n",
    ");\n",
    "\n",
    "#Now we will load the csv file into this new table created in our database\n",
    "\n",
    "load data local infile '/home/ubuntu/Checkout.csv' into table checkout fields terminated by ',' Enclosed by '\"' Lines terminated by '\\n' Ignore 1 rows;\n",
    "\n",
    "load data local infile '/home/ubuntu/Inventory.csv' \n",
    "into table inventory fields terminated by ',' Enclosed by '\"' Lines terminated by '\\n' Ignore 1 rows;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will establish a connection with database and write an api which will perform two functions\n",
    "#1 it will recommend book to users based on their genre preference\n",
    "#2 it will help them find book by author even if they do not specify full name\n",
    "\n",
    "#Establishing connection with the database\n",
    "host=\"18.223.3.204\" #host ip adress\n",
    "port=3306 #port on which we will establish a connection\n",
    "dbname=\"project\" #name of database\n",
    "user=\"root\" #username for accessing db\n",
    "password=\"host\" #password for accessing db\n",
    "\n",
    "conn = pymysql.connect(host, user=user,port=port,\n",
    "                           passwd=password, db=dbname) #create a connection between python api and aws instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "new=pd.DataFrame(pd.read_sql('select * from checkout', con=conn))\n",
    "a=pd.DataFrame(pd.read_sql('select * from inventory', con=conn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre=raw_input(\"Enter the year and genre you want to have statistics for: \")\n",
    "\n",
    "def statistics(year, genre):\n",
    "\n",
    "#We will find the statistics of which book was most checked out in each genre\n",
    "#Performing groupby operation to remove duplicate values\n",
    "    s=new[(new['Year']==year)&& (new[genre]==genre)].groupby('BibNumber').sum().sort_values('count',ascending=False)\n",
    "#removing duplicate value\n",
    "    s.drop_duplicates(inplace=True)\n",
    "#merging to get information of Author and book tile\n",
    "    s=pd.merge(left=s,right=pd.DataFrame(new[['BibNumber','Author','Title']]),left_on='BibNumber',right_on='BibNumber')\n",
    "#display entire row\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "#Ourput the result\n",
    "    Print('The most checked out book in fiction was: ')\n",
    "    return pd.DataFrame(s[['Title','Author']].head(1))\n",
    "\n",
    "#We will find the statistics like which book was least checked out in each genre\n",
    "#Performing groupby operation to remove duplicate values\n",
    "    s=new[(new['Year']==year)&& (new[genre]==genre)].groupby('BibNumber').sum().sort_values('count',ascending=True)\n",
    "#removing duplicate value\n",
    "    s.drop_duplicates(inplace=True)\n",
    "#merging to get information of Author and book tile\n",
    "    s=pd.merge(left=s,right=pd.DataFrame(new[['BibNumber','Author','Title']]),left_on='BibNumber',right_on='BibNumber')\n",
    "#display entire row\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "#Ourput the result\n",
    "    Print('The least checked out book in fiction was: ')\n",
    "    return pd.DataFrame(s[['Title','Author']].head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recommending books to users based on checkout records\n",
    "\n",
    "#Taking input for genre\n",
    "\n",
    "genre=raw_input(\"Enter the genre you want to have suggestions for: \")\n",
    "\n",
    "def suggest(genre):\n",
    "\n",
    "#We will find the book which were checkedout most in users mentioned genre and recommend top 10 books to user\n",
    "    s=new[new[genre]==1].groupby('BibNumber').sum().sort_values('count',ascending=False)\n",
    "    s=pd.merge(left=s,right=pd.DataFrame(new[['BibNumber','Author','Title']]),left_on='BibNumber',right_on='BibNumber')\n",
    "    s.drop_duplicates(inplace=True)\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    return pd.DataFrame(s[['Title','Author']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding books for user based on author name even if partial\n",
    "\n",
    "#Taking input for author\n",
    "\n",
    "author=raw_input(\"Enter the author you want to search for: \")\n",
    "\n",
    "#Function to find books based on author\n",
    "def find(author):\n",
    "    #Using iterrows to traverese each row in datafram\n",
    "    for index,row in a.iterrows():\n",
    "        #Condition to check if author name is matching with user keywords\n",
    "        if re.search(str(author),str(row['Author'])):\n",
    "            pd.set_option('display.max_colwidth', -1) #display entire cell\n",
    "            print(pd.DataFrame(row[['Title','Author','Publisher']])) #Print title,author and publish for matching authors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
